module name=matmul, target=hexagon-32-noos-hvx-hvx_128-hvx_v66-no_asserts-no_bounds_query
internal func matmul_par_for_column_sums_b_s0_x_x (__user_context, column_sums_b.s0.x.x, closure_arg) {
let closure_prototype = (void *)make_struct(reinterpret<(void *)>((uint64)0), reinterpret<(void *)>((uint64)0), 0, 0, 0)
let column_sums_b = (void *)load_typed_struct_member((void *)closure_arg, closure_prototype, 0)
let mat_b = (void *)load_typed_struct_member((void *)closure_arg, closure_prototype, 1)
let mat_a.extent.0 = load_typed_struct_member((void *)closure_arg, closure_prototype, 2)
let mat_b.stride.1 = load_typed_struct_member((void *)closure_arg, closure_prototype, 3)
let t114 = load_typed_struct_member((void *)closure_arg, closure_prototype, 4)
if (column_sums_b.s0.x.x < t114) {
 allocate sum$1[uint32 * 128 * 1]
 produce sum$1 {
  sum$1[ramp(0, 1, 128)] = x128((uint32)0)
  let t115 = column_sums_b.s0.x.x*128
  for (sum$1.s1.fk$x, 0, mat_a.extent.0) {
   sum$1[ramp(0, 1, 128)] = (uint32x128)widen_right_add(sum$1[ramp(0, 1, 128)], uint16x128(mat_b[ramp((mat_b.stride.1*sum$1.s1.fk$x) + t115, 1, 128)]))
  }
 }
 consume sum$1 {
  column_sums_b[ramp(column_sums_b.s0.x.x*128, 1, 128) aligned(128, 0)] = sum$1[ramp(0, 1, 128)]
 }
 free sum$1
} else if ((column_sums_b.s0.x.x + 1) <= t114) {
 allocate sum$1[uint32 * 128 * 1]
 produce sum$1 {
  sum$1[ramp(0, 1, 128)] = x128((uint32)0)
  let t116 = column_sums_b.s0.x.x*128
  for (sum$1.s1.fk$x, 0, mat_a.extent.0) {
   sum$1[ramp(0, 1, 128)] = (uint32x128)widen_right_add(sum$1[ramp(0, 1, 128)], uint16x128(mat_b[ramp((mat_b.stride.1*sum$1.s1.fk$x) + t116, 1, 128)]))
  }
 }
 consume sum$1 {
  column_sums_b[ramp(column_sums_b.s0.x.x*128, 1, 128) aligned(128, 0)] = sum$1[ramp(0, 1, 128)]
 }
 free sum$1
} else {
 allocate sum$1[uint32 * 128 * 1]
 produce sum$1 {
  predicate (ramp((column_sums_b.s0.x.x*128) + 1, 1, 128) <= x128(t114*128))
   sum$1[ramp(0, 1, 128)] = x128((uint32)0)
  let t118 = t114*128
  let t117 = column_sums_b.s0.x.x*128
  for (sum$1.s1.fk$x, 0, mat_a.extent.0) {
   let t84 = ramp(t117 + 1, 1, 128) <= x128(t118)
   predicate (ramp(t117 + 1, 1, 128) <= x128(t118))
    sum$1[ramp(0, 1, 128)] = (uint32x128)widen_right_add(sum$1[ramp(0, 1, 128)] if t84, uint16x128((mat_b[ramp((mat_b.stride.1*sum$1.s1.fk$x) + t117, 1, 128)] if t84)))
  }
 }
 consume sum$1 {
  predicate (ramp((column_sums_b.s0.x.x*128) + 1, 1, 128) <= x128(t114*128))
   column_sums_b[ramp(column_sums_b.s0.x.x*128, 1, 128) aligned(128, 0)] = sum$1[ramp(0, 1, 128)] if (ramp((column_sums_b.s0.x.x*128) + 1, 1, 128) <= x128(t114*128))
 }
 free sum$1
}
}


internal func matmul_par_for_row_sums_a_s0_y_y (__user_context, row_sums_a.s0.y.y, closure_arg$1) {
let closure_prototype$1 = (void *)make_struct(reinterpret<(void *)>((uint64)0), reinterpret<(void *)>((uint64)0), 0, 0, 0, 0, 0)
let mat_a = (void *)load_typed_struct_member((void *)closure_arg$1, closure_prototype$1, 0)
let row_sums_a = (void *)load_typed_struct_member((void *)closure_arg$1, closure_prototype$1, 1)
let mat_a.extent.0 = load_typed_struct_member((void *)closure_arg$1, closure_prototype$1, 2)
let mat_a.extent.1 = load_typed_struct_member((void *)closure_arg$1, closure_prototype$1, 3)
let mat_a.stride.1 = load_typed_struct_member((void *)closure_arg$1, closure_prototype$1, 4)
let t120 = load_typed_struct_member((void *)closure_arg$1, closure_prototype$1, 5)
let t121 = load_typed_struct_member((void *)closure_arg$1, closure_prototype$1, 6)
let row_sums_a.s0.y.yi.base = min(row_sums_a.s0.y.y*32, t120 + -32)
let t123 = row_sums_a.s0.y.yi.base - t121
for (row_sums_a.s0.y.yi, 0, 32) {
 if (let t144 = (row_sums_a.s0.y.yi + row_sums_a.s0.y.yi.base) in ((-1 <= t144) && ((t144 + 2) <= mat_a.extent.1))) {
  (uint8)prefetch(mat_a, ((row_sums_a.s0.y.yi + row_sums_a.s0.y.yi.base) + 1)*mat_a.stride.1, mat_a.extent.0, 1, 1, mat_a.stride.1)
 }
 allocate sum[uint32 * 1]
 produce sum {
  sum[0] = (uint32)0
  let t124 = (row_sums_a.s0.y.yi + row_sums_a.s0.y.yi.base)*mat_a.stride.1
  for (sum.s1.fk$x, 0, mat_a.extent.0) {
   sum[0] = sum[0] + uint32(mat_a[sum.s1.fk$x + t124])
  }
 }
 consume sum {
  row_sums_a[(row_sums_a.s0.y.yi + t123) + 32] = sum[0]
 }
 free sum
}
}


internal func matmul_par_for_output_s0_x_xo (__user_context, output.s0.x.xo, closure_arg$2) {
let closure_prototype$2 = (void *)make_struct(reinterpret<(void *)>((uint64)0), reinterpret<(void *)>((uint64)0), reinterpret<(void *)>((uint64)0), reinterpret<(void *)>((uint64)0), reinterpret<(void *)>((uint64)0), reinterpret<(void *)>((uint64)0), 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, (int16)0, (int16)0, (uint8)0, (uint8)0)
let bias = (void *)load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 0)
let column_sums_b = (void *)load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 1)
let mat_a = (void *)load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 2)
let mat_b = (void *)load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 3)
let output = (void *)load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 4)
let row_sums_a = (void *)load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 5)
let mat_a.extent.0 = load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 6)
let mat_a.extent.1 = load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 7)
let mat_a.stride.1 = load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 8)
let mat_b.stride.1 = load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 9)
let output.stride.1 = load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 10)
let output_multiplier = load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 11)
let output_offset = load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 12)
let output_shift = load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 13)
let t131 = load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 14)
let t132 = load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 15)
let t133 = load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 16)
let t129 = (int16)load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 17)
let t130 = (int16)load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 18)
let output_max = (uint8)load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 19)
let output_min = (uint8)load_typed_struct_member((void *)closure_arg$2, closure_prototype$2, 20)
allocate mat_b_swizzled[uint8 * 128 * t132 * 4]
produce mat_b_swizzled {
 let t135 = output.s0.x.xo*128
 let t134 = output.s0.x.xo*32
 for (mat_b_swizzled.s0.y, 0, t132) {
  mat_b_swizzled[ramp(mat_b_swizzled.s0.y*512, 1, 512) aligned(512, 0)] = interleave_vectors(mat_b[ramp(((mat_b.stride.1*mat_b_swizzled.s0.y) + t134)*4, 1, 128) aligned(4, 0)], mat_b[ramp((((mat_b_swizzled.s0.y*4) + 1)*mat_b.stride.1) + t135, 1, 128)], mat_b[ramp((((mat_b_swizzled.s0.y*4) + 2)*mat_b.stride.1) + t135, 1, 128) aligned(2, 0)], mat_b[ramp((((mat_b_swizzled.s0.y*4) + 3)*mat_b.stride.1) + t135, 1, 128)])
 }
}
consume mat_b_swizzled {
 let t137 = output.s0.x.xo*128
 let t138 = output.s0.x.xo*32
 for (output.s0.y.yo, 0, t133) {
  if (((output.s0.y.yo*4) + 8) <= mat_a.extent.1) {
   (uint8)prefetch(mat_a, ((output.s0.y.yo + 1)*mat_a.stride.1)*4, t132*4, 1, 4, mat_a.stride.1)
  }
  allocate multiplied_no_offsets[uint32 * 128 * 4]
  produce multiplied_no_offsets {
   multiplied_no_offsets[ramp(0, 1, 128)] = x128((uint32)0)
   multiplied_no_offsets[ramp(128, 1, 128)] = x128((uint32)0)
   multiplied_no_offsets[ramp(256, 1, 128)] = x128((uint32)0)
   multiplied_no_offsets[ramp(384, 1, 128)] = x128((uint32)0)
   let t143 = output.s0.y.yo*4
   let t139 = mat_a.stride.1*output.s0.y.yo
   let t142 = (t143 + 3)*mat_a.stride.1
   let t141 = (t143 + 2)*mat_a.stride.1
   let t140 = (t143 + 1)*mat_a.stride.1
   for (multiplied_no_offsets.s1.k$x, 0, t132) {
    let t86 = mat_b_swizzled[ramp(multiplied_no_offsets.s1.k$x*512, 1, 512) aligned(512, 0)]
    let t103 = multiplied_no_offsets.s1.k$x + t139
    multiplied_no_offsets[ramp(0, 1, 128)] = (multiplied_no_offsets[ramp(0, 1, 128)] + (uint32x128)widening_add((uint16x128)widening_mul(slice_vectors(t86, 0, 4, 128), x128(mat_a[t103*4])), (uint16x128)widening_mul(slice_vectors(t86, 1, 4, 128), x128(mat_a[(t103*4) + 1])))) + (uint32x128)widening_add((uint16x128)widening_mul(slice_vectors(t86, 2, 4, 128), x128(mat_a[(t103*4) + 2])), (uint16x128)widening_mul(slice_vectors(t86, 3, 4, 128), x128(mat_a[(t103*4) + 3])))
    let t88 = mat_b_swizzled[ramp(multiplied_no_offsets.s1.k$x*512, 1, 512) aligned(512, 0)]
    let t89 = (multiplied_no_offsets.s1.k$x*4) + t140
    multiplied_no_offsets[ramp(128, 1, 128)] = (multiplied_no_offsets[ramp(128, 1, 128)] + (uint32x128)widening_add((uint16x128)widening_mul(slice_vectors(t88, 0, 4, 128), x128(mat_a[t89])), (uint16x128)widening_mul(slice_vectors(t88, 1, 4, 128), x128(mat_a[t89 + 1])))) + (uint32x128)widening_add((uint16x128)widening_mul(slice_vectors(t88, 2, 4, 128), x128(mat_a[t89 + 2])), (uint16x128)widening_mul(slice_vectors(t88, 3, 4, 128), x128(mat_a[t89 + 3])))
    let t90 = mat_b_swizzled[ramp(multiplied_no_offsets.s1.k$x*512, 1, 512) aligned(512, 0)]
    let t91 = (multiplied_no_offsets.s1.k$x*4) + t141
    multiplied_no_offsets[ramp(256, 1, 128)] = (multiplied_no_offsets[ramp(256, 1, 128)] + (uint32x128)widening_add((uint16x128)widening_mul(slice_vectors(t90, 0, 4, 128), x128(mat_a[t91])), (uint16x128)widening_mul(slice_vectors(t90, 1, 4, 128), x128(mat_a[t91 + 1])))) + (uint32x128)widening_add((uint16x128)widening_mul(slice_vectors(t90, 2, 4, 128), x128(mat_a[t91 + 2])), (uint16x128)widening_mul(slice_vectors(t90, 3, 4, 128), x128(mat_a[t91 + 3])))
    let t92 = mat_b_swizzled[ramp(multiplied_no_offsets.s1.k$x*512, 1, 512) aligned(512, 0)]
    let t93 = (multiplied_no_offsets.s1.k$x*4) + t142
    multiplied_no_offsets[ramp(384, 1, 128)] = (multiplied_no_offsets[ramp(384, 1, 128)] + (uint32x128)widening_add((uint16x128)widening_mul(slice_vectors(t92, 0, 4, 128), x128(mat_a[t93])), (uint16x128)widening_mul(slice_vectors(t92, 1, 4, 128), x128(mat_a[t93 + 1])))) + (uint32x128)widening_add((uint16x128)widening_mul(slice_vectors(t92, 2, 4, 128), x128(mat_a[t93 + 2])), (uint16x128)widening_mul(slice_vectors(t92, 3, 4, 128), x128(mat_a[t93 + 3])))
   }
  }
  consume multiplied_no_offsets {
   let t104 = int32(t129)
   let t105 = int32(t130)
   output[ramp(((output.s0.y.yo*output.stride.1) + t138)*4, 1, 128) aligned(16, 0)] = max(min((uint8x128)saturating_cast((int32x128)rounding_shift_right((int32x128)rounding_mul_shift_right(((int32x128(column_sums_b[ramp(t137, 1, 128) aligned(128, 0)])*x128(t104)) + (bias[ramp(t137, 1, 128) aligned(128, 0)] + int32x128(multiplied_no_offsets[ramp(0, 1, 128)]))) + x128(((t104*t105)*mat_a.extent.0) + (t105*int32(row_sums_a[((output.s0.y.yo - t131)*4) + 32]))), x128(output_multiplier), x128((uint32)31)), x128(output_shift)) + x128(output_offset)), x128(output_max)), x128(output_min))
   let t106 = int32(t129)
   let t107 = int32(t130)
   output[ramp((((output.s0.y.yo*4) + 1)*output.stride.1) + t137, 1, 128) aligned(4, 0)] = max(min((uint8x128)saturating_cast((int32x128)rounding_shift_right((int32x128)rounding_mul_shift_right(((int32x128(column_sums_b[ramp(t137, 1, 128) aligned(128, 0)])*x128(t106)) + (bias[ramp(t137, 1, 128) aligned(128, 0)] + int32x128(multiplied_no_offsets[ramp(128, 1, 128)]))) + x128(((t106*t107)*mat_a.extent.0) + (t107*int32(row_sums_a[((output.s0.y.yo - t131)*4) + 33]))), x128(output_multiplier), x128((uint32)31)), x128(output_shift)) + x128(output_offset)), x128(output_max)), x128(output_min))
   let t108 = int32(t129)
   let t109 = int32(t130)
   output[ramp((((output.s0.y.yo*4) + 2)*output.stride.1) + t137, 1, 128) aligned(8, 0)] = max(min((uint8x128)saturating_cast((int32x128)rounding_shift_right((int32x128)rounding_mul_shift_right(((int32x128(column_sums_b[ramp(t137, 1, 128) aligned(128, 0)])*x128(t108)) + (bias[ramp(t137, 1, 128) aligned(128, 0)] + int32x128(multiplied_no_offsets[ramp(256, 1, 128)]))) + x128(((t108*t109)*mat_a.extent.0) + (t109*int32(row_sums_a[((output.s0.y.yo - t131)*4) + 34]))), x128(output_multiplier), x128((uint32)31)), x128(output_shift)) + x128(output_offset)), x128(output_max)), x128(output_min))
   let t110 = int32(t129)
   let t111 = int32(t130)
   output[ramp((((output.s0.y.yo*4) + 3)*output.stride.1) + t137, 1, 128) aligned(4, 0)] = max(min((uint8x128)saturating_cast((int32x128)rounding_shift_right((int32x128)rounding_mul_shift_right(((int32x128(column_sums_b[ramp(t137, 1, 128) aligned(128, 0)])*x128(t110)) + (bias[ramp(t137, 1, 128) aligned(128, 0)] + int32x128(multiplied_no_offsets[ramp(384, 1, 128)]))) + x128(((t110*t111)*mat_a.extent.0) + (t111*int32(row_sums_a[((output.s0.y.yo - t131)*4) + 35]))), x128(output_multiplier), x128((uint32)31)), x128(output_shift)) + x128(output_offset)), x128(output_max)), x128(output_min))
   free multiplied_no_offsets
  }
 }
}
free mat_b_swizzled
}


external_plus_metadata func matmul (mat_a, mat_b, bias, mat_a_offset, mat_b_offset, output_multiplier, output_shift, output_offset, output_min, output_max, output) {
let bias = (void *)_halide_buffer_get_host((struct halide_buffer_t *)bias.buffer)
let mat_a = (void *)_halide_buffer_get_host((struct halide_buffer_t *)mat_a.buffer)
let mat_a.extent.0 = _halide_buffer_get_extent((struct halide_buffer_t *)mat_a.buffer, 0)
let mat_a.extent.1 = _halide_buffer_get_extent((struct halide_buffer_t *)mat_a.buffer, 1)
let mat_a.stride.1 = _halide_buffer_get_stride((struct halide_buffer_t *)mat_a.buffer, 1)
let mat_b = (void *)_halide_buffer_get_host((struct halide_buffer_t *)mat_b.buffer)
let mat_b.stride.1 = _halide_buffer_get_stride((struct halide_buffer_t *)mat_b.buffer, 1)
let output = (void *)_halide_buffer_get_host((struct halide_buffer_t *)output.buffer)
let output.extent.0 = _halide_buffer_get_extent((struct halide_buffer_t *)output.buffer, 0)
let output.extent.1 = _halide_buffer_get_extent((struct halide_buffer_t *)output.buffer, 1)
let output.stride.1 = _halide_buffer_get_stride((struct halide_buffer_t *)output.buffer, 1)
allocate column_sums_b[uint32 * ((output.extent.0/128)*128)]
produce output {
 produce column_sums_b {
  let t114 = output.extent.0/128
  let parallel_closure = (void *)make_struct(column_sums_b, mat_b, mat_a.extent.0, mat_b.stride.1, t114)
  let closure_result = halide_do_par_for((halide_task_t)::matmul_par_for_column_sums_b_s0_x_x, 0, t114, (uint8_t *)(parallel_closure))
  assert(closure_result == 0, closure_result)
 }
 allocate row_sums_a[uint32 * (max(output.extent.1/4, 8)*4)]
 produce row_sums_a {
  let t122 = output.extent.1/4
  let t119 = (output.extent.1 + 28)/32
  let t121 = min(t122, 8)*4
  let t120 = t122*4
  let parallel_closure$1 = (void *)make_struct(mat_a, row_sums_a, mat_a.extent.0, mat_a.extent.1, mat_a.stride.1, t120, t121)
  let closure_result$1 = halide_do_par_for((halide_task_t)::matmul_par_for_row_sums_a_s0_y_y, 0, t119, (uint8_t *)(parallel_closure$1))
  assert(closure_result$1 == 0, closure_result$1)
 }
 consume row_sums_a {
  consume column_sums_b {
   let t132 = mat_a.extent.0/4
   let t133 = output.extent.1/4
   let t131 = min(t133, 8)
   let t130 = min((int16)mat_b_offset, (int16)0)
   let t129 = min((int16)mat_a_offset, (int16)0)
   let t125 = output.extent.0/128
   let parallel_closure$2 = (void *)make_struct(bias, column_sums_b, mat_a, mat_b, output, row_sums_a, mat_a.extent.0, mat_a.extent.1, mat_a.stride.1, mat_b.stride.1, output.stride.1, output_multiplier, output_offset, output_shift, t131, t132, t133, t129, t130, (uint8)output_max, (uint8)output_min)
   let closure_result$2 = halide_do_par_for((halide_task_t)::matmul_par_for_output_s0_x_xo, 0, t125, (uint8_t *)(parallel_closure$2))
   assert(closure_result$2 == 0, closure_result$2)
  }
 }
 free column_sums_b
 free row_sums_a
}
}


